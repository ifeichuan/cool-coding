---
title: PyTorch åˆ†å¸ƒå¼è®­ç»ƒåŸºç¡€
description: äº†è§£PyTorchåˆ†å¸ƒå¼è®­ç»ƒçš„åŸºæœ¬æ¦‚å¿µã€å®ç°æ–¹æ³•ä»¥åŠå®é™…åº”ç”¨åœºæ™¯ï¼Œé€‚åˆåˆå­¦è€…å¿«é€Ÿå…¥é—¨ã€‚
---

# PyTorch åˆ†å¸ƒå¼è®­ç»ƒåŸºç¡€

åœ¨ç°ä»£æ·±åº¦å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹çš„è§„æ¨¡å’Œæ•°æ®é›†çš„å¤§å°éƒ½åœ¨å¿«é€Ÿå¢é•¿ã€‚å•æœºå•å¡çš„è®­ç»ƒæ–¹å¼å·²ç»æ— æ³•æ»¡è¶³éœ€æ±‚ï¼Œåˆ†å¸ƒå¼è®­ç»ƒæˆä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜çš„å…³é”®æŠ€æœ¯ã€‚PyTorch æä¾›äº†å¼ºå¤§çš„åˆ†å¸ƒå¼è®­ç»ƒå·¥å…·ï¼Œå¸®åŠ©å¼€å‘è€…é«˜æ•ˆåœ°åˆ©ç”¨å¤šå°æœºå™¨å’Œå¤šä¸ª GPU è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚æœ¬æ–‡å°†ä»‹ç» PyTorch åˆ†å¸ƒå¼è®­ç»ƒçš„åŸºç¡€çŸ¥è¯†ï¼Œå¹¶é€šè¿‡å®é™…æ¡ˆä¾‹å¸®åŠ©ä½ å¿«é€Ÿä¸Šæ‰‹ã€‚

## ä»€ä¹ˆæ˜¯åˆ†å¸ƒå¼è®­ç»ƒï¼Ÿ

åˆ†å¸ƒå¼è®­ç»ƒæ˜¯æŒ‡å°†è®­ç»ƒä»»åŠ¡åˆ†é…åˆ°å¤šä¸ªè®¡ç®—èŠ‚ç‚¹ï¼ˆå¦‚å¤šå°æœºå™¨æˆ–å¤šä¸ª GPUï¼‰ä¸Šå¹¶è¡Œæ‰§è¡Œï¼Œä»¥åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚PyTorch æä¾›äº†å¤šç§åˆ†å¸ƒå¼è®­ç»ƒçš„æ–¹å¼ï¼ŒåŒ…æ‹¬æ•°æ®å¹¶è¡Œï¼ˆData Parallelismï¼‰ã€æ¨¡å‹å¹¶è¡Œï¼ˆModel Parallelismï¼‰å’Œæ··åˆå¹¶è¡Œï¼ˆHybrid Parallelismï¼‰ã€‚

:::note
**æ•°æ®å¹¶è¡Œ**ï¼šå°†æ•°æ®åˆ†ç‰‡ï¼Œæ¯ä¸ªè®¡ç®—èŠ‚ç‚¹å¤„ç†ä¸€éƒ¨åˆ†æ•°æ®ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒæ­¥æ¨¡å‹å‚æ•°ã€‚
**æ¨¡å‹å¹¶è¡Œ**ï¼šå°†æ¨¡å‹åˆ†ç‰‡ï¼Œæ¯ä¸ªè®¡ç®—èŠ‚ç‚¹è´Ÿè´£æ¨¡å‹çš„ä¸€éƒ¨åˆ†è®¡ç®—ã€‚
**æ··åˆå¹¶è¡Œ**ï¼šç»“åˆæ•°æ®å¹¶è¡Œå’Œæ¨¡å‹å¹¶è¡Œçš„ä¼˜åŠ¿ï¼Œé€‚ç”¨äºè¶…å¤§è§„æ¨¡æ¨¡å‹ã€‚
:::

## PyTorch åˆ†å¸ƒå¼è®­ç»ƒçš„æ ¸å¿ƒç»„ä»¶

PyTorch åˆ†å¸ƒå¼è®­ç»ƒçš„æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬ï¼š

1. **`torch.distributed` æ¨¡å—**ï¼šæä¾›äº†åˆ†å¸ƒå¼è®­ç»ƒçš„åŸºç¡€åŠŸèƒ½ï¼Œå¦‚è¿›ç¨‹ç»„ç®¡ç†ã€é€šä¿¡åŸè¯­ç­‰ã€‚
2. **`torch.nn.parallel.DistributedDataParallel` (DDP)**ï¼šç”¨äºå®ç°æ•°æ®å¹¶è¡Œçš„åˆ†å¸ƒå¼è®­ç»ƒã€‚
3. **`torch.distributed.launch` è„šæœ¬**ï¼šç”¨äºå¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒä»»åŠ¡ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†é€æ­¥è®²è§£å¦‚ä½•ä½¿ç”¨è¿™äº›ç»„ä»¶è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒã€‚

---

## 1. åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ

åœ¨å¼€å§‹åˆ†å¸ƒå¼è®­ç»ƒä¹‹å‰ï¼Œéœ€è¦åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒã€‚PyTorch æä¾›äº† `torch.distributed.init_process_group` å‡½æ•°æ¥å®Œæˆè¿™ä¸€ä»»åŠ¡ã€‚

```python
import torch.distributed as dist

def init_distributed(backend='nccl', world_size=2, rank=0, master_addr='localhost', master_port='12355'):
    dist.init_process_group(
        backend=backend,
        init_method=f'tcp://{master_addr}:{master_port}',
        world_size=world_size,
        rank=rank
    )
```

:::tip
- `backend`ï¼šæŒ‡å®šé€šä¿¡åç«¯ï¼Œå¸¸ç”¨çš„æ˜¯ `nccl`ï¼ˆé€‚ç”¨äº GPUï¼‰å’Œ `gloo`ï¼ˆé€‚ç”¨äº CPUï¼‰ã€‚
- `world_size`ï¼šå‚ä¸è®­ç»ƒçš„æ€»è¿›ç¨‹æ•°ã€‚
- `rank`ï¼šå½“å‰è¿›ç¨‹çš„ç¼–å·ï¼ˆä» 0 å¼€å§‹ï¼‰ã€‚
- `master_addr` å’Œ `master_port`ï¼šä¸»èŠ‚ç‚¹çš„åœ°å€å’Œç«¯å£ã€‚
:::

---

## 2. ä½¿ç”¨ DistributedDataParallel (DDP) è¿›è¡Œæ•°æ®å¹¶è¡Œè®­ç»ƒ

`DistributedDataParallel` æ˜¯ PyTorch ä¸­ç”¨äºæ•°æ®å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒçš„æ ¸å¿ƒå·¥å…·ã€‚å®ƒä¼šåœ¨æ¯ä¸ªè¿›ç¨‹ä¸­å¤åˆ¶æ¨¡å‹ï¼Œå¹¶åœ¨åå‘ä¼ æ’­æ—¶åŒæ­¥æ¢¯åº¦ã€‚

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.nn.parallel import DistributedDataParallel as DDP

class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc = nn.Linear(10, 1)

    def forward(self, x):
        return self.fc(x)

def train(rank, world_size):
    init_distributed(world_size=world_size, rank=rank)

    model = SimpleModel().to(rank)
    ddp_model = DDP(model, device_ids=[rank])

    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)
    criterion = nn.MSELoss()

    for _ in range(10):
        inputs = torch.randn(20, 10).to(rank)
        labels = torch.randn(20, 1).to(rank)

        outputs = ddp_model(inputs)
        loss = criterion(outputs, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    dist.destroy_process_group()

if __name__ == "__main__":
    world_size = 2
    torch.multiprocessing.spawn(train, args=(world_size,), nprocs=world_size)
```

:::caution
- ä½¿ç”¨ `DDP` æ—¶ï¼Œæ¯ä¸ªè¿›ç¨‹éœ€è¦åŠ è½½ä¸åŒçš„æ•°æ®åˆ†ç‰‡ã€‚
- è®­ç»ƒå®Œæˆåï¼Œè°ƒç”¨ `dist.destroy_process_group()` æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒã€‚
:::

---

## 3. å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒä»»åŠ¡

PyTorch æä¾›äº† `torch.distributed.launch` è„šæœ¬æ¥ç®€åŒ–åˆ†å¸ƒå¼è®­ç»ƒçš„å¯åŠ¨ã€‚å‡è®¾ä½ çš„è®­ç»ƒè„šæœ¬åä¸º `train.py`ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒï¼š

```bash
python -m torch.distributed.launch --nproc_per_node=2 train.py
```

:::note
- `--nproc_per_node`ï¼šæŒ‡å®šæ¯ä¸ªèŠ‚ç‚¹ä½¿ç”¨çš„ GPU æ•°é‡ã€‚
- å¦‚æœä½ çš„è®­ç»ƒä»»åŠ¡éœ€è¦è·¨å¤šä¸ªèŠ‚ç‚¹ï¼Œè¿˜éœ€è¦è®¾ç½® `--nnodes` å’Œ `--node_rank` å‚æ•°ã€‚
:::

---

## 4. å®é™…æ¡ˆä¾‹ï¼šåˆ†å¸ƒå¼è®­ç»ƒ MNIST åˆ†ç±»æ¨¡å‹

ä»¥ä¸‹æ˜¯ä¸€ä¸ªå®Œæ•´çš„åˆ†å¸ƒå¼è®­ç»ƒ MNIST åˆ†ç±»æ¨¡å‹çš„ç¤ºä¾‹ï¼š

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.nn.parallel import DistributedDataParallel as DDP
import torch.distributed as dist
from torchvision import datasets, transforms

class MNISTModel(nn.Module):
    def __init__(self):
        super(MNISTModel, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = x.view(-1, 28 * 28)
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

def train(rank, world_size):
    init_distributed(world_size=world_size, rank=rank)

    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
    dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    sampler = torch.utils.data.distributed.DistributedSampler(dataset)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, sampler=sampler)

    model = MNISTModel().to(rank)
    ddp_model = DDP(model, device_ids=[rank])

    optimizer = optim.Adam(ddp_model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()

    for epoch in range(5):
        sampler.set_epoch(epoch)
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(rank), labels.to(rank)

            outputs = ddp_model(inputs)
            loss = criterion(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    dist.destroy_process_group()

if __name__ == "__main__":
    world_size = 2
    torch.multiprocessing.spawn(train, args=(world_size,), nprocs=world_size)
```

:::tip
- ä½¿ç”¨ `DistributedSampler` ç¡®ä¿æ¯ä¸ªè¿›ç¨‹åŠ è½½ä¸åŒçš„æ•°æ®åˆ†ç‰‡ã€‚
- åœ¨æ¯ä¸ª epoch å¼€å§‹æ—¶è°ƒç”¨ `sampler.set_epoch(epoch)`ï¼Œä»¥ç¡®ä¿æ•°æ®æ‰“ä¹±çš„éšæœºæ€§ã€‚
:::

---

## æ€»ç»“

æœ¬æ–‡ä»‹ç»äº† PyTorch åˆ†å¸ƒå¼è®­ç»ƒçš„åŸºç¡€çŸ¥è¯†ï¼ŒåŒ…æ‹¬åˆ†å¸ƒå¼ç¯å¢ƒçš„åˆå§‹åŒ–ã€`DistributedDataParallel` çš„ä½¿ç”¨ä»¥åŠå¦‚ä½•å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒä»»åŠ¡ã€‚é€šè¿‡å®é™…æ¡ˆä¾‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•åœ¨ MNIST æ•°æ®é›†ä¸Šè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒã€‚

åˆ†å¸ƒå¼è®­ç»ƒæ˜¯æ·±åº¦å­¦ä¹ é¢†åŸŸçš„é‡è¦æŠ€æœ¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡è®­ç»ƒæ•ˆç‡ã€‚å¸Œæœ›æœ¬æ–‡èƒ½å¸®åŠ©ä½ å¿«é€Ÿå…¥é—¨ PyTorch åˆ†å¸ƒå¼è®­ç»ƒï¼Œå¹¶ä¸ºä½ çš„æ·±åº¦å­¦ä¹ é¡¹ç›®æä¾›æ”¯æŒã€‚

---

## é™„åŠ èµ„æºä¸ç»ƒä¹ 

1. **å®˜æ–¹æ–‡æ¡£**ï¼šé˜…è¯» [PyTorch åˆ†å¸ƒå¼è®­ç»ƒå®˜æ–¹æ–‡æ¡£](https://pytorch.org/docs/stable/distributed.html) ä»¥äº†è§£æ›´å¤šé«˜çº§åŠŸèƒ½ã€‚
2. **ç»ƒä¹ **ï¼šå°è¯•åœ¨ CIFAR-10 æ•°æ®é›†ä¸Šå®ç°åˆ†å¸ƒå¼è®­ç»ƒï¼Œå¹¶æ¯”è¾ƒå•æœºè®­ç»ƒå’Œåˆ†å¸ƒå¼è®­ç»ƒçš„é€Ÿåº¦å·®å¼‚ã€‚
3. **æ‰©å±•é˜…è¯»**ï¼šäº†è§£ PyTorch çš„æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰å¦‚ä½•ä¸åˆ†å¸ƒå¼è®­ç»ƒç»“åˆä½¿ç”¨ï¼Œä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

Happy coding! ğŸš€